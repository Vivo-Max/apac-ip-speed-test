import logging
import sys
import os
import requests
import re
import csv
import subprocess
import threading
import time
import json
import argparse
import platform
import shutil
import tarfile
from typing import List, Tuple, Dict
from collections import defaultdict
from charset_normalizer import detect
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from pathlib import Path
from packaging import version
import tempfile
import atexit
import stat
import venv
import ast

# ç¡®ä¿æ—¥å¿—æ–‡ä»¶è·¯å¾„å¯å†™
LOG_FILE = "speedtest.log"
LOG_DIR = os.path.dirname(os.path.abspath(__file__))
try:
    os.makedirs(LOG_DIR, exist_ok=True)
    LOG_PATH = os.path.join(LOG_DIR, LOG_FILE)
    with open(LOG_PATH, 'a', encoding='utf-8') as f:
        pass
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(LOG_PATH, encoding="utf-8", mode="w"),
            logging.StreamHandler(sys.stdout)
        ],
        force=True
    )
    logger = logging.getLogger(__name__)
    logger.debug(f"æ—¥å¿—åˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {LOG_PATH}")
except Exception as e:
    print(f"æ— æ³•åˆ›å»ºæ—¥å¿—æ–‡ä»¶ {LOG_PATH}: {e}")
    sys.exit(1)

# ç¦ç”¨ stdout ç¼“å†²ï¼Œç¡®ä¿æ—¥å¿—å®æ—¶è¾“å‡º
sys.stdout.reconfigure(line_buffering=True)

# é…ç½®
IP_LIST_FILE = "ip.txt"
IPS_FILE = "ips.txt"
FINAL_CSV = "ip.csv"
INPUT_FILE = "input.csv"
TEMP_FILE = os.path.join(tempfile.gettempdir(), "temp_proxy.csv")
TEMP_FILE_CACHE_DURATION = 3600
INPUT_URL = "https://bihai.cf/CFIP/CUCC/standard.csv"
COUNTRY_CACHE_FILE = "country_cache.json"
GEOIP_DB_PATH = Path("GeoLite2-Country.mmdb")
GEOIP_DB_URL_BACKUP = "https://download.maxmind.com/app/geoip_download?edition_id=GeoLite2-Country&license_key={}&suffix=tar.gz"
MAXMIND_LICENSE_KEY = os.getenv("MAXMIND_LICENSE_KEY", "")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://www.google.com/"
}
DESIRED_COUNTRIES = ['TW', 'JP', 'HK', 'SG', 'KR', 'IN', 'KP', 'VN', 'TH', 'MM']
REQUIRED_PACKAGES = ['requests', 'charset-normalizer', 'geoip2==4.8.0', 'maxminddb>=2.0.0']
CONFIG_FILE = ".gitconfig.json"
SSH_KEY_PATH = os.path.expanduser("~/.ssh/id_ed25519")
VENV_DIR = ".venv"

COUNTRY_LABELS = {
    'JP': ('ğŸ‡¯ğŸ‡µ', 'æ—¥æœ¬'), 'KR': ('ğŸ‡°ğŸ‡·', 'éŸ©å›½'), 'SG': ('ğŸ‡¸ğŸ‡¬', 'æ–°åŠ å¡'),
    'TW': ('ğŸ‡¹ğŸ‡¼', 'å°æ¹¾'), 'HK': ('ğŸ‡­ğŸ‡°', 'é¦™æ¸¯'), 'MY': ('ğŸ‡²ğŸ‡¾', 'é©¬æ¥è¥¿äºš'),
    'TH': ('ğŸ‡¹ğŸ‡­', 'æ³°å›½'), 'ID': ('ğŸ‡®ğŸ‡©', 'å°åº¦å°¼è¥¿äºš'), 'PH': ('ğŸ‡µğŸ‡­', 'è²å¾‹å®¾'),
    'VN': ('ğŸ‡»ğŸ‡³', 'è¶Šå—'), 'IN': ('ğŸ‡®ğŸ‡³', 'å°åº¦'), 'MO': ('ğŸ‡²ğŸ‡´', 'æ¾³é—¨'),
    'KH': ('ğŸ‡°ğŸ‡­', 'æŸ¬åŸ”å¯¨'), 'LA': ('ğŸ‡±ğŸ‡¦', 'è€æŒ'), 'MM': ('ğŸ‡²ğŸ‡²', 'ç¼…ç”¸'),
    'MN': ('ğŸ‡²ğŸ‡³', 'è’™å¤'), 'KP': ('ğŸ‡°ğŸ‡µ', 'æœé²œ'), 'US': ('ğŸ‡ºğŸ‡¸', 'ç¾å›½'),
    'GB': ('ğŸ‡¬ğŸ‡§', 'è‹±å›½'), 'DE': ('ğŸ‡©ğŸ‡ª', 'å¾·å›½'), 'FR': ('ğŸ‡«ğŸ‡·', 'æ³•å›½'),
    'IT': ('ğŸ‡®ğŸ‡¹', 'æ„å¤§åˆ©'), 'ES': ('ğŸ‡ªğŸ‡¸', 'è¥¿ç­ç‰™'), 'NL': ('ğŸ‡³ğŸ‡±', 'è·å…°'),
    'FI': ('ğŸ‡«ğŸ‡®', 'èŠ¬å…°'), 'AU': ('ğŸ‡¦ğŸ‡º', 'æ¾³å¤§åˆ©äºš'), 'CA': ('ğŸ‡¨ğŸ‡¦', 'åŠ æ‹¿å¤§'),
    'NZ': ('ğŸ‡³ğŸ‡¿', 'æ–°è¥¿å…°'), 'BR': ('ğŸ‡§ğŸ‡·', 'å·´è¥¿'), 'RU': ('ğŸ‡·ğŸ‡º', 'ä¿„ç½—æ–¯'),
    'PL': ('ğŸ‡µğŸ‡±', 'æ³¢å…°'), 'UA': ('ğŸ‡ºğŸ‡¦', 'ä¹Œå…‹å…°'), 'CZ': ('ğŸ‡¨ğŸ‡¿', 'æ·å…‹'),
    'HU': ('ğŸ‡­ğŸ‡º', 'åŒˆç‰™åˆ©'), 'RO': ('ğŸ‡·ğŸ‡´', 'ç½—é©¬å°¼äºš'), 'SA': ('ğŸ‡¸ğŸ‡¦', 'æ²™ç‰¹é˜¿æ‹‰ä¼¯'),
    'AE': ('ğŸ‡¦ğŸ‡ª', 'é˜¿è”é…‹'), 'QA': ('ğŸ‡¶ğŸ‡¦', 'å¡å¡”å°”'), 'IL': ('ğŸ‡®ğŸ‡±', 'ä»¥è‰²åˆ—'),
    'TR': ('ğŸ‡¹ğŸ‡·', 'åœŸè€³å…¶'), 'IR': ('ğŸ‡®ğŸ‡·', 'ä¼Šæœ—'),
    'CN': ('ğŸ‡¨ğŸ‡³', 'ä¸­å›½'), 'BD': ('ğŸ‡§ğŸ‡©', 'å­ŸåŠ æ‹‰å›½'), 'PK': ('ğŸ‡µğŸ‡°', 'å·´åŸºæ–¯å¦'),
    'LK': ('ğŸ‡±ğŸ‡°', 'æ–¯é‡Œå…°å¡'), 'NP': ('ğŸ‡µğŸ‡µ', 'å°¼æ³Šå°”'), 'BT': ('ğŸ‡§ğŸ‡¹', 'ä¸ä¸¹'),
    'MV': ('ğŸ‡²ğŸ‡»', 'é©¬å°”ä»£å¤«'), 'BN': ('ğŸ‡§ğŸ‡³', 'æ–‡è±'), 'TL': ('ğŸ‡¹ğŸ‡±', 'ä¸œå¸æ±¶'),
    'EG': ('ğŸ‡ªğŸ‡¬', 'åŸƒåŠ'), 'ZA': ('ğŸ‡¿ğŸ‡¦', 'å—é'), 'NG': ('ğŸ‡³ğŸ‡¬', 'å°¼æ—¥åˆ©äºš'),
    'KE': ('ğŸ‡°ğŸ‡ª', 'è‚¯å°¼äºš'), 'GH': ('ğŸ‡¬ğŸ‡­', 'åŠ çº³'), 'MA': ('ğŸ‡²ğŸ‡¦', 'æ‘©æ´›å“¥'),
    'DZ': ('ğŸ‡©ğŸ‡¿', 'é˜¿å°”åŠåˆ©äºš'), 'TN': ('ğŸ‡¹ğŸ‡³', 'çªå°¼æ–¯'), 'AR': ('ğŸ‡¦ğŸ‡·', 'é˜¿æ ¹å»·'),
    'CL': ('ğŸ‡¨ğŸ‡±', 'æ™ºåˆ©'), 'CO': ('ğŸ‡¨ğŸ‡´', 'å“¥ä¼¦æ¯”äºš'), 'PE': ('ğŸ‡µğŸ‡ª', 'ç§˜é²'),
    'MX': ('ğŸ‡²ğŸ‡½', 'å¢¨è¥¿å“¥'), 'VE': ('ğŸ‡»ğŸ‡ª', 'å§”å†…ç‘æ‹‰'), 'SE': ('ğŸ‡¸ğŸ‡ª', 'ç‘å…¸'),
    'NO': ('ğŸ‡³ğŸ‡´', 'æŒªå¨'), 'DK': ('ğŸ‡©ğŸ‡°', 'ä¸¹éº¦'), 'CH': ('ğŸ‡¨ğŸ‡­', 'ç‘å£«'),
    'AT': ('ğŸ‡¦ğŸ‡¹', 'å¥¥åœ°åˆ©'), 'BE': ('ğŸ‡§ğŸ‡ª', 'æ¯”åˆ©æ—¶'), 'IE': ('ğŸ‡®ğŸ‡ª', 'çˆ±å°”å…°'),
    'PT': ('ğŸ‡µğŸ‡¹', 'è‘¡è„ç‰™'), 'GR': ('ğŸ‡¬ğŸ‡·', 'å¸Œè…Š'), 'BG': ('ğŸ‡§ğŸ‡¬', 'ä¿åŠ åˆ©äºš'),
    'SK': ('ğŸ‡¸ğŸ‡°', 'æ–¯æ´›ä¼å…‹'), 'SI': ('ğŸ‡¸ğŸ‡®', 'æ–¯æ´›æ–‡å°¼äºš'), 'HR': ('ğŸ‡­ğŸ‡·', 'å…‹ç½—åœ°äºš'),
    'RS': ('ğŸ‡·ğŸ‡¸', 'å¡å°”ç»´äºš'), 'BA': ('ğŸ‡§ğŸ‡¦', 'æ³¢é»‘'), 'MK': ('ğŸ‡²ğŸ‡°', 'åŒ—é©¬å…¶é¡¿'),
    'AL': ('ğŸ‡¦ğŸ‡±', 'é˜¿å°”å·´å°¼äºš'), 'KZ': ('ğŸ‡°ğŸ‡¿', 'å“ˆè¨å…‹æ–¯å¦'), 'UZ': ('ğŸ‡ºğŸ‡¿', 'ä¹Œå…¹åˆ«å…‹æ–¯å¦'),
    'KG': ('ğŸ‡°ğŸ‡¬', 'å‰å°”å‰æ–¯æ–¯å¦'), 'TJ': ('ï¿½TJ', 'å¡”å‰å…‹æ–¯å¦'), 'TM': ('ğŸ‡¹ğŸ‡²', 'åœŸåº“æ›¼æ–¯å¦'),
    'GE': ('ğŸ‡¬ğŸ‡ª', 'æ ¼é²å‰äºš'), 'AM': ('ğŸ‡¦ğŸ‡²', 'äºšç¾å°¼äºš'), 'AZ': ('ğŸ‡¦ğŸ‡¿', 'é˜¿å¡æ‹œç–†'),
    'KW': ('ğŸ‡°ğŸ‡¼', 'ç§‘å¨ç‰¹'), 'BH': ('ğŸ‡§ğŸ‡­', 'å·´æ—'), 'OM': ('ğŸ‡´ğŸ‡²', 'é˜¿æ›¼'),
    'JO': ('ğŸ‡¯ğŸ‡´', 'çº¦æ—¦'), 'LB': ('ğŸ‡±ğŸ‡§', 'é»å·´å«©'), 'SY': ('ğŸ‡¸ğŸ‡¾', 'å™åˆ©äºš'),
    'IQ': ('ğŸ‡®ğŸ‡¶', 'ä¼Šæ‹‰å…‹'), 'YE': ('ğŸ‡¾ğŸ‡ª', 'ä¹Ÿé—¨'),
    'EE': ('ğŸ‡ªğŸ‡ª', 'çˆ±æ²™å°¼äºš'), 'LV': ('ğŸ‡±ğŸ‡»', 'æ‹‰è„±ç»´äºš'), 'LT': ('ğŸ‡±ğŸ‡¹', 'ç«‹é™¶å®›')
}
COUNTRY_ALIASES = {
    'SOUTH KOREA': 'KR', 'KOREA': 'KR', 'REPUBLIC OF KOREA': 'KR', 'KOREA, REPUBLIC OF': 'KR',
    'HONG KONG': 'HK', 'HONGKONG': 'HK', 'HK SAR': 'HK',
    'UNITED STATES': 'US', 'USA': 'US', 'U.S.': 'US', 'UNITED STATES OF AMERICA': 'US',
    'UNITED KINGDOM': 'GB', 'UK': 'GB', 'GREAT BRITAIN': 'GB', 'è‹±å›½': 'GB',
    'JAPAN': 'JP', 'JPN': 'JP', 'æ—¥æœ¬': 'JP',
    'TAIWAN': 'TW', 'TWN': 'TW', 'TAIWAN, PROVINCE OF CHINA': 'TW', 'å°æ¹¾': 'TW',
    'SINGAPORE': 'SG', 'SGP': 'SG', 'æ–°åŠ å¡': 'SG',
    'FRANCE': 'FR', 'FRA': 'FR', 'æ³•å›½': 'FR',
    'GERMANY': 'DE', 'DEU': 'DE', 'å¾·å›½': 'DE',
    'NETHERLANDS': 'NL', 'NLD': 'NL', 'è·å…°': 'NL',
    'AUSTRALIA': 'AU', 'AUS': 'AU', 'æ¾³å¤§åˆ©äºš': 'AU',
    'CANADA': 'CA', 'CAN': 'CA', 'åŠ æ‹¿å¤§': 'CA',
    'BRAZIL': 'BR', 'BRA': 'BR', 'å·´è¥¿': 'BR',
    'RUSSIA': 'RU', 'RUS': 'RU', 'ä¿„ç½—æ–¯': 'RU',
    'INDIA': 'IN', 'IND': 'IN', 'å°åº¦': 'IN',
    'CHINA': 'CN', 'CHN': 'CN', 'ä¸­å›½': 'CN',
    'VIET NAM': 'VN', 'VIETNAM': 'VN', 'è¶Šå—': 'VN',
    'THAILAND': 'TH', 'THA': 'TH', 'æ³°å›½': 'TH',
    'BURMA': 'MM', 'MYANMAR': 'MM', 'ç¼…ç”¸': 'MM',
    'NORTH KOREA': 'KP', 'KOREA, DEMOCRATIC PEOPLE\'S REPUBLIC OF': 'KP', 'æœé²œ': 'KP'
}

def find_speedtest_script() -> str:
    system = platform.system().lower()
    candidates = []
    if system == "windows":
        candidates = ["iptest.bat", ".\\iptest.bat"]
    else:
        candidates = ["iptest.sh", "./iptest.sh", "iptest", "./iptest"]
    for candidate in candidates:
        if os.path.exists(candidate):
            if not os.access(candidate, os.X_OK) and system != "windows":
                try:
                    os.chmod(candidate, 0o755)
                    logger.info(f"å·²ä¸º {candidate} æ·»åŠ æ‰§è¡Œæƒé™")
                except Exception as e:
                    logger.error(f"æ— æ³•ä¸º {candidate} æ·»åŠ æ‰§è¡Œæƒé™: {e}")
                    continue
            logger.info(f"æ‰¾åˆ°æµ‹é€Ÿè„šæœ¬: {candidate}")
            return candidate
    logger.error("æœªæ‰¾åˆ°æµ‹é€Ÿè„šæœ¬ï¼Œè¯·ç¡®ä¿ iptest.sh æˆ– iptest.bat å­˜åœ¨")
    sys.exit(1)

SPEEDTEST_SCRIPT = find_speedtest_script()

geoip_reader = None

def cleanup_temp_file():
    if os.path.exists(TEMP_FILE):
        try:
            os.remove(TEMP_FILE)
            logger.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {TEMP_FILE}")
        except Exception as e:
            logger.warning(f"æ— æ³•æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {e}")

atexit.register(cleanup_temp_file)

def setup_and_activate_venv():
    logger = logging.getLogger(__name__)
    
    STATIC_REQUIRED_PACKAGES = ['requests', 'charset-normalizer', 'geoip2==4.8.0', 'maxminddb>=2.0.0']
    
    def get_non_stdlib_imports(script_path):
        stdlib_modules = set(sys.stdlib_module_names)
        imports = set()
        try:
            with open(script_path, 'r', encoding='utf-8') as f:
                tree = ast.parse(f.read(), filename=script_path)
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for name in node.names:
                        module = name.name.split('.')[0]
                        if module not in stdlib_modules:
                            imports.add(module)
                elif isinstance(node, ast.ImportFrom):
                    module = node.module.split('.')[0] if node.module else None
                    if module and module not in stdlib_modules:
                        imports.add(module)
        except Exception as e:
            logger.warning(f"è§£æè„šæœ¬ä¾èµ–å¤±è´¥: {e}")
        return imports
    
    MODULE_TO_PACKAGE = {
        'requests': 'requests>=2.32.3',
        'charset_normalizer': 'charset-normalizer>=3.4.1',
        'geoip2': 'geoip2==4.8.0',
        'maxminddb': 'maxminddb>=2.0.0',
        'packaging': 'packaging>=21.3',
    }
    
    script_path = os.path.abspath(__file__)
    dynamic_imports = get_non_stdlib_imports(script_path)
    logger.debug(f"åŠ¨æ€æ£€æµ‹åˆ°çš„éæ ‡å‡†åº“æ¨¡å—: {dynamic_imports}")
    
    REQUIRED_PACKAGES = list(STATIC_REQUIRED_PACKAGES)
    for module in dynamic_imports:
        if module in MODULE_TO_PACKAGE and MODULE_TO_PACKAGE[module] not in REQUIRED_PACKAGES:
            REQUIRED_PACKAGES.append(MODULE_TO_PACKAGE[module])
    logger.debug(f"æœ€ç»ˆä¾èµ–åˆ—è¡¨: {REQUIRED_PACKAGES}")
    
    system = sys.platform.lower()
    if system.startswith('win'):
        system = 'windows'
    elif system.startswith('linux'):
        system = 'linux'
    elif system.startswith('darwin'):
        system = 'darwin'
    else:
        logger.error(f"ä¸æ”¯æŒçš„å¹³å°: {system}")
        sys.exit(1)
    
    logger.debug(f"æ£€æµ‹åˆ°çš„å¹³å°: {system}")
    logger.debug(f"Python å¯æ‰§è¡Œæ–‡ä»¶: {sys.executable}, ç‰ˆæœ¬: {sys.version}")
    
    venv_path = Path('.venv')
    logger.debug(f"è™šæ‹Ÿç¯å¢ƒè·¯å¾„: {venv_path}")
    
    recreate_venv = False
    if venv_path.exists():
        logger.debug(f"æ£€æµ‹åˆ°ç°æœ‰è™šæ‹Ÿç¯å¢ƒ: {venv_path}")
        venv_python = str(venv_path / ('Scripts' if system == 'windows' else 'bin') / 'python')
        try:
            result = subprocess.run([venv_python, '--version'], check=True, capture_output=True, text=True)
            logger.debug(f"è™šæ‹Ÿç¯å¢ƒ Python ç‰ˆæœ¬: {result.stdout.strip()}")
        except subprocess.CalledProcessError as e:
            logger.warning(f"è™šæ‹Ÿç¯å¢ƒ Python ä¸å¯ç”¨: {e}, å°†é‡æ–°åˆ›å»º")
            recreate_venv = True
    else:
        logger.debug("æœªæ‰¾åˆ°è™šæ‹Ÿç¯å¢ƒï¼Œå°†åˆ›å»º")
        recreate_venv = True
    
    pip_venv = str(venv_path / ('Scripts' if system == 'windows' else 'bin') / 'pip')
    logger.debug("å¼€å§‹æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒä¾èµ–")
    installed_packages = {}
    if not recreate_venv:
        try:
            result = subprocess.run([pip_venv, "list", "--format=json"], check=True, capture_output=True, text=True)
            logger.debug(f"pip list è¾“å‡º: {result.stdout}")
            installed_packages = {pkg["name"].lower(): pkg["version"] for pkg in json.loads(result.stdout)}
            logger.debug(f"å·²å®‰è£…çš„åŒ…: {installed_packages}")
        except subprocess.CalledProcessError as e:
            logger.error(f"pip list å¤±è´¥: {e}, è¾“å‡º: {e.output}")
            recreate_venv = True
    
    missing_packages = []
    if not recreate_venv:
        for pkg in REQUIRED_PACKAGES:
            if '==' in pkg:
                pkg_name, expected_version = pkg.split('==')
                version_op = '=='
            elif '>=' in pkg:
                pkg_name, expected_version = pkg.split('>=')
                version_op = '>='
            else:
                pkg_name, expected_version = pkg, None
                version_op = None
            pkg_name = pkg_name.lower().replace('_', '-')
            
            if pkg_name not in installed_packages:
                logger.warning(f"æœªæ‰¾åˆ°ä¾èµ–: {pkg_name}")
                missing_packages.append(pkg)
                continue
            
            if expected_version:
                installed_version = installed_packages[pkg_name]
                if version_op == '==' and installed_version != expected_version:
                    logger.warning(f"ä¾èµ– {pkg_name} ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œå®é™… {installed_version}ï¼ŒæœŸæœ› == {expected_version}")
                    missing_packages.append(pkg)
                elif version_op == '>=' and version.parse(installed_version) < version.parse(expected_version):
                    logger.warning(f"ä¾èµ– {pkg_name} ç‰ˆæœ¬è¿‡ä½ï¼Œå®é™… {installed_version}ï¼ŒæœŸæœ› >= {expected_version}")
                    missing_packages.append(pkg)
    
    if missing_packages:
        logger.warning(f"è™šæ‹Ÿç¯å¢ƒç¼ºå°‘ä¾èµ–: {missing_packages}ï¼Œå°†é‡æ–°åˆ›å»º")
        recreate_venv = True
    else:
        logger.info("æ‰€æœ‰ä¾èµ–å·²æ»¡è¶³ï¼Œæ— éœ€é‡æ–°åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ")
        recreate_venv = False
    
    if recreate_venv:
        if venv_path.exists():
            logger.debug("åˆ é™¤ç°æœ‰è™šæ‹Ÿç¯å¢ƒ")
            shutil.rmtree(venv_path, ignore_errors=True)
            logger.debug("æˆåŠŸåˆ é™¤ç°æœ‰è™šæ‹Ÿç¯å¢ƒ")
        
        logger.debug(f"åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ: {venv_path}")
        subprocess.run([sys.executable, '-m', 'venv', str(venv_path)], check=True)
        logger.debug("è™šæ‹Ÿç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        
        venv_python = str(venv_path / ('Scripts' if system == 'windows' else 'bin') / 'python')
        pip_venv = str(venv_path / ('Scripts' if system == 'windows' else 'bin') / 'pip')
        logger.debug(f"è™šæ‹Ÿç¯å¢ƒ Python: {venv_python}, pip: {pip_venv}")
        
        try:
            result = subprocess.run([pip_venv, 'install', '--upgrade', 'pip'], check=True, capture_output=True, text=True)
            logger.debug(f"å‡çº§ pip æˆåŠŸ: {result.stdout}")
        except subprocess.CalledProcessError as e:
            logger.warning(f"å‡çº§ pip å¤±è´¥: {e}, è¾“å‡º: {e.output}")
        
        for pkg in REQUIRED_PACKAGES:
            logger.debug(f"å®‰è£…ä¾èµ–: {pkg}")
            try:
                result = subprocess.run([pip_venv, 'install', pkg], check=True, capture_output=True, text=True)
                logger.debug(f"æˆåŠŸå®‰è£…ä¾èµ–: {pkg}, è¾“å‡º: {result.stdout}")
            except subprocess.CalledProcessError as e:
                logger.error(f"å®‰è£…ä¾èµ– {pkg} å¤±è´¥: {e}, è¾“å‡º: {e.output}")
                sys.exit(1)
    
    venv_site = str(venv_path / ('Lib' if system == 'windows' else 'lib') / 
                    f"python{sys.version_info.major}.{sys.version_info.minor}" / 'site-packages')
    logger.debug(f"è™šæ‹Ÿç¯å¢ƒ site-packages: {venv_site}")
    if venv_site not in sys.path:
        sys.path.insert(0, venv_site)
    logger.debug("è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»")
    
    for module in list(sys.modules.keys()):
        if module.startswith('geoip2') or module.startswith('maxminddb'):
            del sys.modules[module]
    logger.debug("å·²æ¸…ç† geoip2 å’Œ maxminddb æ¨¡å—ç¼“å­˜")
    
    try:
        import geoip2
        logger.debug(f"geoip2 æ¨¡å—å·²å¯¼å…¥ï¼Œç‰ˆæœ¬: {geoip2.__version__}")
    except ImportError as e:
        logger.error(f"æ— æ³•å¯¼å…¥ geoip2: {e}", exc_info=True)
        sys.exit(1)
    
    try:
        import geoip2.database
        logger.debug("geoip2.database æ¨¡å—å·²æˆåŠŸå¯¼å…¥")
    except ImportError as e:
        logger.error(f"æ— æ³•å¯¼å…¥ geoip2.database: {e}", exc_info=True)
        sys.exit(1)
    
    try:
        import maxminddb
        logger.debug(f"maxminddb æ¨¡å—å·²å¯¼å…¥ï¼Œç‰ˆæœ¬: {maxminddb.__version__}")
    except ImportError as e:
        logger.error(f"æ— æ³•å¯¼å…¥ maxminddb: {e}", exc_info=True)
        sys.exit(1)
    
    try:
        import packaging
        logger.debug(f"packaging æ¨¡å—å·²å¯¼å…¥ï¼Œç‰ˆæœ¬: {packaging.__version__}")
    except ImportError as e:
        logger.error(f"æ— æ³•å¯¼å…¥ packaging: {e}", exc_info=True)
        sys.exit(1)

def get_latest_geoip_url() -> str:
    api_url = "https://api.github.com/repos/P3TERX/GeoLite.mmdb/releases/latest"
    logger.info(f"æ­£åœ¨ä» GitHub API è·å–æœ€æ–°ç‰ˆæœ¬: {api_url}")
    try:
        session = requests.Session()
        retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504, 429])
        session.mount('https://', HTTPAdapter(max_retries=retry))
        response = session.get(api_url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        release_data = response.json()
        
        for asset in release_data.get("assets", []):
            if asset.get("name") == "GeoLite2-Country.mmdb":
                download_url = asset.get("browser_download_url")
                logger.info(f"æ‰¾åˆ°æœ€æ–° GeoIP æ•°æ®åº“ URL: {download_url}")
                return download_url
        
        logger.error("æœªæ‰¾åˆ° GeoLite2-Country.mmdb çš„ä¸‹è½½ URL")
        return ""
    except Exception as e:
        logger.error(f"æ— æ³•è·å–æœ€æ–° GeoIP æ•°æ®åº“ URL: {e}")
        return ""

def download_geoip_database(dest_path: Path) -> bool:
    url = get_latest_geoip_url()
    if not url:
        logger.error("æ— æ³•è·å–æœ€æ–° GeoIP æ•°æ®åº“ URL")
        return False
    
    # å®šä¹‰å¤šä¸ªä»£ç†æœåŠ¡
    proxy_services = [
        ("Ghfast.top", "https://ghfast.top/"),
        ("Gitproxy.clickr", "https://gitproxy.click/"),
        ("Gh-proxy.ygxz", "https://gh-proxy.ygxz.in/"),
        ("Github.ur1.fun", "https://github.ur1.fun/")
    ]
    
    # é¦–å…ˆå°è¯•ç›´æ¥ä½¿ç”¨åŸå§‹ URLï¼ˆæ— ä»£ç†ï¼‰
    urls_to_try = [("æ— ä»£ç†", url)]
    # ç„¶åæ·»åŠ æ‰€æœ‰ä»£ç†æœåŠ¡
    for proxy_name, proxy_prefix in proxy_services:
        if url.startswith("https://github.com/"):
            proxy_url = proxy_prefix + url
            urls_to_try.append((proxy_name, proxy_url))
    
    for proxy_name, download_url in urls_to_try:
        logger.info(f"ä¸‹è½½ GeoIP æ•°æ®åº“ï¼ˆä½¿ç”¨ {proxy_name}ï¼‰: {download_url}")
        try:
            session = requests.Session()
            retry = Retry(total=5, backoff_factor=2, status_forcelist=[500, 502, 503, 504, 429])
            session.mount('https://', HTTPAdapter(max_retries=retry))
            response = session.get(download_url, timeout=60, stream=True, headers=HEADERS)
            response.raise_for_status()
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            with open(dest_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            logger.info(f"ä¸‹è½½è¿›åº¦: {progress:.2f}%")
            logger.info(f"GeoIP æ•°æ®åº“ä¸‹è½½å®Œæˆ: {dest_path}")
            if not dest_path.exists() or dest_path.stat().st_size < 100:
                logger.error(f"ä¸‹è½½çš„ GeoIP æ•°æ®åº“æ— æ•ˆ")
                dest_path.unlink(missing_ok=True)
                return False
            return True
        except Exception as e:
            logger.warning(f"é€šè¿‡ {proxy_name} ä¸‹è½½ GeoIP æ•°æ®åº“å¤±è´¥: {e}")
            continue
    
    logger.error("æ‰€æœ‰ä»£ç†æœåŠ¡å‡æ— æ³•ä¸‹è½½ GeoIP æ•°æ®åº“")
    return False

def download_geoip_database_maxmind(dest_path: Path) -> bool:
    if not MAXMIND_LICENSE_KEY:
        logger.warning("æœªè®¾ç½® MAXMIND_LICENSE_KEYï¼Œæ— æ³•ä» MaxMind ä¸‹è½½ GeoIP æ•°æ®åº“ã€‚è¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® MAXMIND_LICENSE_KEY æˆ–æ£€æŸ¥ GitHub ä¸‹è½½æºã€‚")
        return False
    url = GEOIP_DB_URL_BACKUP.format(MAXMIND_LICENSE_KEY)
    logger.info(f"ä» MaxMind ä¸‹è½½ GeoIP æ•°æ®åº“: {url}")
    try:
        # åˆ é™¤æ—§æ•°æ®åº“æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if dest_path.exists():
            logger.info(f"åˆ é™¤æ—§çš„ GeoIP æ•°æ®åº“æ–‡ä»¶: {dest_path}")
            dest_path.unlink(missing_ok=True)
            
        session = requests.Session()
        retry = Retry(total=5, backoff_factor=2, status_forcelist=[500, 502, 503, 504, 429])
        session.mount('https://', HTTPAdapter(max_retries=retry))
        response = session.get(url, timeout=60, stream=True, headers=HEADERS)
        response.raise_for_status()
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        temp_tar = dest_path.with_suffix(".tar.gz")
        with open(temp_tar, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        logger.info(f"ä¸‹è½½è¿›åº¦: {progress:.2f}%")
        with tarfile.open(temp_tar, "r:gz") as tar:
            for member in tar.getmembers():
                if member.name.endswith("GeoLite2-Country.mmdb"):
                    tar.extract(member, dest_path.parent)
                    extracted_path = dest_path.parent / member.name
                    extracted_path.rename(dest_path)
                    break
        temp_tar.unlink(missing_ok=True)
        if not dest_path.exists() or dest_path.stat().st_size < 100:
            logger.error(f"è§£å‹çš„ GeoIP æ•°æ®åº“æ— æ•ˆ")
            dest_path.unlink(missing_ok=True)
            return False
        return True
    except Exception as e:
        logger.error(f"ä» MaxMind ä¸‹è½½ GeoIP æ•°æ®åº“å¤±è´¥: {e}")
        temp_tar.unlink(missing_ok=True)
        return False

def init_geoip_reader(offline: bool = False, update_geoip: bool = False):
    global geoip_reader
    
    def is_geoip_file_valid(file_path: Path) -> bool:
        if not file_path.exists():
            return False
        if file_path.stat().st_size < 1024 * 1024:  # å°äº 1MB
            logger.warning(f"GeoIP æ•°æ®åº“æ–‡ä»¶ {file_path} è¿‡å°ï¼Œå¯èƒ½æ— æ•ˆ")
            return False
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¿‡æœŸï¼ˆä¾‹å¦‚ 30 å¤©ï¼‰
        mtime = file_path.stat().st_mtime
        current_time = time.time()
        age_days = (current_time - mtime) / (24 * 3600)
        if age_days > 30:
            logger.warning(f"GeoIP æ•°æ®åº“æ–‡ä»¶ {file_path} å·²è¶…è¿‡ 30 å¤© ({age_days:.1f} å¤©)ï¼Œå»ºè®®ä½¿ç”¨ --update-geoip æ›´æ–°")
        return True
    
    # å¦‚æœæ˜¯ç¦»çº¿æ¨¡å¼ï¼Œç›´æ¥æ£€æŸ¥æœ¬åœ°æ•°æ®åº“
    if offline:
        logger.info("ç¦»çº¿æ¨¡å¼å¯ç”¨ï¼Œå°†ä½¿ç”¨æœ¬åœ° GeoIP æ•°æ®åº“")
        if not GEOIP_DB_PATH.exists():
            logger.error(f"ç¦»çº¿æ¨¡å¼ä¸‹æœªæ‰¾åˆ°æœ¬åœ° GeoIP æ•°æ®åº“: {GEOIP_DB_PATH}")
            sys.exit(1)
    else:
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶æ›´æ–°
        if update_geoip:
            logger.info("æ£€æµ‹åˆ° --update-geoip å‚æ•°ï¼Œå¼ºåˆ¶æ›´æ–° GeoIP æ•°æ®åº“")
            GEOIP_DB_PATH.unlink(missing_ok=True)
        # æ£€æŸ¥æœ¬åœ°æ•°æ®åº“æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
        if GEOIP_DB_PATH.exists() and is_geoip_file_valid(GEOIP_DB_PATH):
            logger.info(f"æœ¬åœ° GeoIP æ•°æ®åº“å·²å­˜åœ¨ä¸”æœ‰æ•ˆ: {GEOIP_DB_PATH}ï¼Œç›´æ¥ä½¿ç”¨")
        else:
            if GEOIP_DB_PATH.exists():
                logger.info(f"æœ¬åœ° GeoIP æ•°æ®åº“æ— æ•ˆ: {GEOIP_DB_PATH}ï¼Œå°†é‡æ–°ä¸‹è½½")
                GEOIP_DB_PATH.unlink(missing_ok=True)
            else:
                logger.info(f"æœ¬åœ° GeoIP æ•°æ®åº“ä¸å­˜åœ¨: {GEOIP_DB_PATH}ï¼Œå°è¯•ä¸‹è½½æœ€æ–°æ–‡ä»¶")
            success = download_geoip_database(GEOIP_DB_PATH)
            if not success:
                logger.warning("ä¸»ä¸‹è½½æºå¤±è´¥ï¼Œå°è¯• MaxMind")
                success = download_geoip_database_maxmind(GEOIP_DB_PATH)
                if not success:
                    logger.error("ä¸‹è½½ GeoIP æ•°æ®åº“å¤±è´¥ï¼Œä¸”æœ¬åœ°æ— å¯ç”¨æ•°æ®åº“")
                    sys.exit(1)
    
    # åŠ è½½æ•°æ®åº“
    try:
        import geoip2.database
        logger.debug("geoip2.database æ¨¡å—å·²å¯¼å…¥")
        with geoip2.database.Reader(GEOIP_DB_PATH) as reader:
            logger.info("GeoIP æ•°æ®åº“éªŒè¯æˆåŠŸ")
        geoip_reader = geoip2.database.Reader(GEOIP_DB_PATH)
        logger.info("GeoIP æ•°æ®åº“åŠ è½½æˆåŠŸ")
    except ImportError as e:
        logger.error(f"æ— æ³•å¯¼å…¥ geoip2.database: {e}. è¯·ç¡®ä¿ geoip2==4.8.0 å·²å®‰è£…ï¼Œå¹¶æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"GeoIP æ•°æ®åº“åŠ è½½å¤±è´¥: {e}, ç±»å‹: {type(e).__name__}")
        if offline:
            logger.error("ç¦»çº¿æ¨¡å¼ä¸‹æ— æ³•åŠ è½½ GeoIP æ•°æ®åº“ï¼Œé€€å‡º")
            sys.exit(1)
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œå°è¯•é‡æ–°ä¸‹è½½
        logger.info("æœ¬åœ°æ•°æ®åº“å¯èƒ½æŸåï¼Œå°è¯•é‡æ–°ä¸‹è½½ GeoIP æ•°æ®åº“")
        GEOIP_DB_PATH.unlink(missing_ok=True)
        success = download_geoip_database(GEOIP_DB_PATH)
        if not success:
            logger.warning("ä¸»ä¸‹è½½æºå¤±è´¥ï¼Œå°è¯• MaxMind")
            success = download_geoip_database_maxmind(GEOIP_DB_PATH)
            if not success:
                logger.error("é‡æ–°ä¸‹è½½ GeoIP æ•°æ®åº“å¤±è´¥")
                sys.exit(1)
        with geoip2.database.Reader(GEOIP_DB_PATH) as reader:
            logger.info("GeoIP æ•°æ®åº“éªŒè¯æˆåŠŸ")
        geoip_reader = geoip2.database.Reader(GEOIP_DB_PATH)
        logger.info("GeoIP æ•°æ®åº“åŠ è½½æˆåŠŸ")

def close_geoip_reader():
    global geoip_reader
    if geoip_reader:
        try:
            geoip_reader.close()
            logger.info("GeoIP æ•°æ®åº“å·²å…³é—­")
        except Exception as e:
            logger.warning(f"å…³é—­ GeoIP æ•°æ®åº“å¤±è´¥: {e}")
        geoip_reader = None

atexit.register(close_geoip_reader)

def check_dependencies(offline: bool = False, update_geoip: bool = False):
    init_geoip_reader(offline=offline, update_geoip=update_geoip)

def load_country_cache() -> Dict[str, str]:
    if os.path.exists(COUNTRY_CACHE_FILE):
        try:
            with open(COUNTRY_CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½å›½å®¶ç¼“å­˜: {e}")
    return {}

def save_country_cache(cache: Dict[str, str]):
    try:
        with open(COUNTRY_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.warning(f"æ— æ³•ä¿å­˜å›½å®¶ç¼“å­˜: {e}")

def is_temp_file_valid(temp_file: str) -> bool:
    if not os.path.exists(temp_file):
        return False
    mtime = os.path.getmtime(temp_file)
    current_time = time.time()
    if (current_time - mtime) > TEMP_FILE_CACHE_DURATION:
        logger.info(f"ä¸´æ—¶æ–‡ä»¶ {temp_file} å·²è¿‡æœŸ")
        return False
    if os.path.getsize(temp_file) < 10:
        logger.warning(f"ä¸´æ—¶æ–‡ä»¶ {temp_file} å†…å®¹å¤ªå°")
        return False
    return True

def detect_delimiter(lines: List[str]) -> str:
    comma_count = sum(1 for line in lines[:5] if ',' in line and line.strip())
    semicolon_count = sum(1 for line in lines[:5] if ';' in line and line.strip())
    tab_count = sum(1 for line in lines[:5] if '\t' in line and line.strip())
    space_count = sum(1 for line in lines[:5] if ' ' in line and line.strip())
    if comma_count > max(semicolon_count, tab_count, space_count) and comma_count > 0:
        return ','
    elif semicolon_count > max(comma_count, tab_count, space_count) and semicolon_count > 0:
        return ';'
    elif tab_count > max(comma_count, semicolon_count, space_count) and tab_count > 0:
        return '\t'
    elif space_count > max(comma_count, semicolon_count, tab_count) and space_count > 0:
        return ' '
    return None

def is_valid_ip(ip: str) -> bool:
    ipv4_pattern = re.compile(r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$')
    ipv6_pattern = re.compile(r'^(?:[0-9a-fA-F]{0,4}:){1,7}[0-9a-fA-F]{0,4}$')
    return bool(ipv4_pattern.match(ip) or ipv6_pattern.match(ip.strip('[]')))

def is_valid_port(port: str) -> bool:
    try:
        port_num = int(port)
        return 0 <= port_num <= 65535
    except (ValueError, TypeError):
        return False

def is_country_like(value: str) -> bool:
    if not value:
        return False
    value_upper = value.upper().strip()
    if re.match(r'^[A-Z]{2}$', value_upper) and value_upper in COUNTRY_LABELS:
        return True
    if value_upper in COUNTRY_ALIASES:
        return True
    return False

def standardize_country(country: str) -> str:
    if not country:
        return ''
    country_clean = re.sub(r'[^a-zA-Z\s]', '', country).strip().upper()
    if country_clean in COUNTRY_LABELS:
        return country_clean
    if country_clean in COUNTRY_ALIASES:
        return COUNTRY_ALIASES[country_clean]
    country_clean = country_clean.replace(' ', '')
    for alias, code in COUNTRY_ALIASES.items():
        alias_clean = alias.replace(' ', '')
        if country_clean == alias_clean:
            return code
    return ''

def find_country_column(lines: List[str], delimiter: str) -> Tuple[int, int, int]:
    country_col = -1
    ip_col = 0
    port_col = 1
    sample_lines = [line for line in lines[:5] if line.strip() and not line.startswith('#')]
    if not sample_lines:
        return ip_col, port_col, country_col

    col_matches = defaultdict(int)
    total_rows = len(sample_lines)
    max_cols = max(len(line.split(delimiter)) for line in sample_lines)

    for line in sample_lines:
        fields = line.split(delimiter)
        for col, field in enumerate(fields):
            field = field.strip()
            if is_country_like(field):
                col_matches[col] += 1

    if col_matches:
        country_col = max(col_matches, key=col_matches.get)
        match_rate = col_matches[country_col] / total_rows
        if match_rate < 0.5:
            country_col = -1
        else:
            logger.info(f"å›½å®¶åˆ—: ç¬¬ {country_col + 1} åˆ— (åŒ¹é…ç‡: {match_rate:.2%})")

    return ip_col, port_col, country_col

def fetch_and_save_to_temp_file(url: str) -> str:
    logger.info(f"ä¸‹è½½ URL: {url} åˆ° {TEMP_FILE}")
    try:
        session = requests.Session()
        retry = Retry(total=5, backoff_factor=2, status_forcelist=[500, 502, 503, 504, 429])
        session.mount('https://', HTTPAdapter(max_retries=retry))
        response = session.get(url, timeout=60, headers=HEADERS, stream=True)
        response.raise_for_status()
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        with open(TEMP_FILE, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        logger.info(f"ä¸‹è½½è¿›åº¦: {progress:.2f}%")
        logger.info(f"å·²ä¸‹è½½åˆ° {TEMP_FILE}")
        return TEMP_FILE
    except Exception as e:
        logger.error(f"æ— æ³•ä¸‹è½½ URL: {e}")
        return ''

def extract_ip_ports_from_file(file_path: str) -> List[Tuple[str, int, str]]:
    if not os.path.exists(file_path):
        logger.error(f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        return []
    start_time = time.time()
    with open(file_path, "rb") as f:
        raw_data = f.read()
    encoding = detect(raw_data).get("encoding", "utf-8")
    try:
        content = raw_data.decode(encoding)
    except UnicodeDecodeError as e:
        logger.error(f"æ— æ³•è§£ç æ–‡ä»¶ {file_path}: {e}")
        return []
    ip_ports = extract_ip_ports_from_content(content)
    logger.info(f"æ–‡ä»¶ {file_path} è§£æå®Œæˆ (è€—æ—¶: {time.time() - start_time:.2f} ç§’)")
    return ip_ports

def extract_ip_ports_from_content(content: str) -> List[Tuple[str, int, str]]:
    server_port_pairs = []
    invalid_lines = []
    content = content.replace('\r\n', '\n').replace('\r', '\n')
    lines = content.splitlines()
    if not lines:
        logger.error("å†…å®¹ä¸ºç©º")
        return []

    logger.info(f"æ•°æ®æºæ ·æœ¬ (å‰ 5 è¡Œ): {lines[:5]}")

    try:
        data = json.loads(content)
        for item in data:
            ip = item.get('ip', '')
            port = item.get('port', '')
            country = standardize_country(
                item.get('country', '') or
                item.get('countryCode', '') or
                item.get('country_code', '') or
                item.get('location', '') or
                item.get('nation', '') or
                item.get('region', '') or
                item.get('geo', '') or
                item.get('area', '')
            )
            if is_valid_ip(ip) and is_valid_port(str(port)):
                server_port_pairs.append((ip, int(port), country))
        logger.info(f"ä» JSON è§£æå‡º {len(server_port_pairs)} ä¸ªèŠ‚ç‚¹ï¼Œå…¶ä¸­ {sum(1 for _, _, c in server_port_pairs if c)} ä¸ªæœ‰å›½å®¶ä¿¡æ¯")
        return list(dict.fromkeys(server_port_pairs))
    except json.JSONDecodeError as e:
        logger.info(f"JSON è§£æå¤±è´¥: {e}")

    delimiter = detect_delimiter(lines)
    if not delimiter:
        logger.warning("æ— æ³•æ£€æµ‹åˆ†éš”ç¬¦ï¼Œå‡å®šä¸ºé€—å·")
        delimiter = ','

    ip_col, port_col, country_col = 0, 1, -1
    lines_to_process = lines
    if lines and lines[0].strip() and not lines[0].startswith('#'):
        header = lines[0].strip().split(delimiter)
        logger.info(f"æ£€æµ‹åˆ°è¡¨å¤´: {header}")
        for idx, col in enumerate(header):
            col_lower = col.strip().lower()
            if col_lower in ['ip', 'address', 'ip_address', 'ipåœ°å€']:
                ip_col = idx
            elif col_lower in ['port', 'ç«¯å£', 'port_number', 'ç«¯å£å·']:
                port_col = idx
            elif col_lower in ['country', 'å›½å®¶', 'country_code', 'countrycode', 'å›½é™…ä»£ç ', 'nation', 'location', 'region', 'geo', 'area']:
                country_col = idx
        if country_col != -1:
            logger.info(f"æ£€æµ‹åˆ°å›½å®¶åˆ—: ç¬¬ {country_col + 1} åˆ— (å­—æ®µå: {header[country_col]})")
            lines_to_process = lines[1:]
        else:
            logger.info("è¡¨å¤´ä¸­ä¸åŒ…å«å›½å®¶åˆ—ï¼Œå°†é€è¡Œé€åˆ—æœç´¢å›½å®¶ä¿¡æ¯")

    ip_port_pattern = re.compile(
        r'(((\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})|\[(?:[0-9a-fA-F]{0,4}:){1,7}[0-9a-fA-F]{0,4}\]|(?:[0-9a-fA-F]{0,4}:){1,7}[0-9a-fA-F]{0,4}))[ :,\t](\d{1,5})'
    )

    for i, line in enumerate(lines_to_process):
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        match = ip_port_pattern.match(line)
        if match:
            server = match.group(1).strip('[]')
            port = match.group(4)
            country = ''
            if delimiter:
                fields = line.split(delimiter)
                if country_col != -1 and country_col < len(fields):
                    country = standardize_country(fields[country_col].strip())
                if not country:
                    for col, field in enumerate(fields):
                        field = field.strip()
                        potential_country = standardize_country(field)
                        if potential_country:
                            country = potential_country
                            logger.info(f"ç¬¬ {i} è¡Œ: ä»ç¬¬ {col + 1} åˆ—æå–å›½å®¶: {field} -> {country}")
                            break
            if is_valid_port(port):
                server_port_pairs.append((server, int(port), country))
            else:
                invalid_lines.append(f"ç¬¬ {i} è¡Œ: {line} (ç«¯å£æ— æ•ˆ)")
            continue
        if delimiter:
            fields = line.split(delimiter)
            if len(fields) < max(ip_col, port_col) + 1:
                invalid_lines.append(f"ç¬¬ {i} è¡Œ: {line} (å­—æ®µå¤ªå°‘)")
                continue
            server = fields[ip_col].strip('[]')
            port_str = fields[port_col].strip()
            country = ''
            if country_col != -1 and country_col < len(fields):
                country = standardize_country(fields[country_col].strip())
            if not country:
                for col, field in enumerate(fields):
                    field = field.strip()
                    potential_country = standardize_country(field)
                    if potential_country:
                        country = potential_country
                        logger.info(f"ç¬¬ {i} è¡Œ: ä»ç¬¬ {col + 1} åˆ—æå–å›½å®¶: {field} -> {country}")
                        break
            if is_valid_ip(server) and is_valid_port(port_str):
                server_port_pairs.append((server, int(port_str), country))
            else:
                invalid_lines.append(f"ç¬¬ {i} è¡Œ: {line} (IP æˆ–ç«¯å£æ— æ•ˆ)")
        else:
            invalid_lines.append(f"ç¬¬ {i} è¡Œ: {line} (æ ¼å¼æ— æ•ˆ)")

    if invalid_lines:
        logger.info(f"å‘ç° {len(invalid_lines)} ä¸ªæ— æ•ˆæ¡ç›®")
    logger.info(f"è§£æå‡º {len(server_port_pairs)} ä¸ªèŠ‚ç‚¹ï¼Œå…¶ä¸­ {sum(1 for _, _, c in server_port_pairs if c)} ä¸ªæœ‰å›½å®¶ä¿¡æ¯")
    unique_server_port_pairs = list(dict.fromkeys(server_port_pairs))
    logger.info(f"å»é‡å: {len(unique_server_port_pairs)} ä¸ªèŠ‚ç‚¹")
    return unique_server_port_pairs

def get_country_from_ip(ip: str, cache: Dict[str, str]) -> str:
    if ip in cache:
        return cache[ip]
    try:
        response = geoip_reader.country(ip)
        country_code = response.country.iso_code or ''
        if country_code:
            cache[ip] = country_code
            return country_code
        return ''
    except Exception:
        return ''

def get_countries_from_ips(ips: List[str], cache: Dict[str, str]) -> List[str]:
    uncached_ips = [ip for ip in ips if ip not in cache]
    if uncached_ips:
        logger.info(f"æ‰¹é‡æŸ¥è¯¢ {len(uncached_ips)} ä¸ª IP çš„å›½å®¶ä¿¡æ¯")
        for ip in uncached_ips:
            try:
                response = geoip_reader.country(ip)
                cache[ip] = response.country.iso_code or ''
            except Exception:
                cache[ip] = ''
    return [cache[ip] for ip in ips]

def write_ip_list(ip_ports: List[Tuple[str, int, str]]) -> str:
    if not ip_ports:
        logger.error(f"æ²¡æœ‰æœ‰æ•ˆçš„èŠ‚ç‚¹æ¥ç”Ÿæˆ {IP_LIST_FILE}")
        return None

    start_time = time.time()
    country_cache = load_country_cache()
    filtered_ip_ports = set()
    country_counts = defaultdict(int)
    filtered_counts = defaultdict(int)
    logger.info(f"å¼€å§‹å¤„ç† {len(ip_ports)} ä¸ªèŠ‚ç‚¹...")

    from_source = sum(1 for _, _, country in ip_ports if country)
    logger.info(f"æ•°æ®æºä¸º {from_source} ä¸ªèŠ‚ç‚¹æä¾›äº†å›½å®¶ä¿¡æ¯")

    ips_to_query = [ip for ip, _, country in ip_ports if not country]
    if ips_to_query:
        logger.info(f"æ‰¹é‡æŸ¥è¯¢ {len(ips_to_query)} ä¸ª IP çš„å›½å®¶ä¿¡æ¯")
        countries = get_countries_from_ips(ips_to_query, country_cache)
        ip_country_map = dict(zip(ips_to_query, countries))
    else:
        ip_country_map = {}

    supplemented = 0
    for ip, port, country in ip_ports:
        final_country = country
        source = "æ•°æ®æº" if country else "å¾…æŸ¥è¯¢"
        
        if not country:
            final_country = ip_country_map.get(ip, '')
            if final_country:
                supplemented += 1
                source = "GeoIP æ•°æ®åº“"
        
        if not DESIRED_COUNTRIES:
            filtered_ip_ports.add((ip, port))
            if final_country:
                country_counts[final_country] += 1
        elif final_country and final_country in DESIRED_COUNTRIES:
            filtered_ip_ports.add((ip, port))
            country_counts[final_country] += 1
        else:
            filtered_counts[final_country or 'UNKNOWN'] += 1

    total_retained = len(filtered_ip_ports)
    total_filtered = sum(filtered_counts.values())
    logger.info(f"è¿‡æ»¤ç»“æœ: ä¿ç•™ {total_retained} ä¸ªèŠ‚ç‚¹ï¼Œè¿‡æ»¤æ‰ {total_filtered} ä¸ªèŠ‚ç‚¹")
    logger.info(f"é€šè¿‡ GeoIP æ•°æ®åº“è¡¥å……å›½å®¶ä¿¡æ¯: {supplemented} ä¸ªèŠ‚ç‚¹")
    logger.info(f"ä¿ç•™çš„å›½å®¶åˆ†å¸ƒ: {dict(country_counts)}")
    logger.info(f"è¿‡æ»¤æ‰çš„å›½å®¶åˆ†å¸ƒ: {dict(filtered_counts)}")

    if not filtered_ip_ports:
        logger.error(f"æ²¡æœ‰æœ‰æ•ˆçš„èŠ‚ç‚¹æ¥ç”Ÿæˆ {IP_LIST_FILE}")
        return None

    with open(IP_LIST_FILE, "w", encoding="utf-8") as f:
        for ip, port in filtered_ip_ports:
            f.write(f"{ip} {port}\n")
    logger.info(f"ç”Ÿæˆ {IP_LIST_FILE}ï¼ŒåŒ…å« {len(filtered_ip_ports)} ä¸ªèŠ‚ç‚¹ (è€—æ—¶: {time.time() - start_time:.2f} ç§’)")
    save_country_cache(country_cache)
    return IP_LIST_FILE

def run_speed_test() -> str:
    if not SPEEDTEST_SCRIPT:
        logger.error("æœªæ‰¾åˆ°æµ‹é€Ÿè„šæœ¬")
        return None
    if not os.path.exists(IP_LIST_FILE):
        logger.error(f"{IP_LIST_FILE} ä¸å­˜åœ¨")
        return None

    start_time = time.time()
    try:
        with open(IP_LIST_FILE, "r", encoding="utf-8") as f:
            ip_lines = [line.strip() for line in f if line.strip()]
        total_nodes = len(ip_lines)
        logger.info(f"{IP_LIST_FILE} åŒ…å« {total_nodes} ä¸ªèŠ‚ç‚¹")
    except Exception as e:
        logger.error(f"æ— æ³•è¯»å– {IP_LIST_FILE}: {e}")
        return None

    logger.info("å¼€å§‹æµ‹é€Ÿ")
    system = platform.system().lower()
    try:
        if system == "windows":
            command = [SPEEDTEST_SCRIPT]
        else:
            shell = shutil.which("bash") or shutil.which("sh") or "sh"
            command = ["stdbuf", "-oL", shell, SPEEDTEST_SCRIPT]
        
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            shell=False,
            encoding='utf-8',
            errors='replace'
        )
        stdout_lines, stderr_lines = [], []
        def read_stream(stream, lines):
            while True:
                line = stream.readline()
                if not line:
                    break
                lines.append(line)
                print(line.strip())
                sys.stdout.flush()
        stdout_thread = threading.Thread(target=read_stream, args=(process.stdout, stdout_lines))
        stderr_thread = threading.Thread(target=read_stream, args=(process.stderr, stderr_lines))
        stdout_thread.start()
        stderr_thread.start()

        return_code = process.wait()
        stdout_thread.join()
        stderr_thread.join()
        stdout = ''.join(stdout_lines)
        stderr = ''.join(stderr_lines)
        if stdout:
            logger.info(f"æµ‹é€Ÿè¾“å‡º: {stdout}")
        if stderr:
            logger.warning(f"æµ‹é€Ÿé”™è¯¯: {stderr}")

        logger.info(f"æµ‹é€Ÿå®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")
        if return_code != 0:
            logger.error(f"æµ‹é€Ÿå¤±è´¥ï¼Œè¿”å›ç : {return_code}")
            return None
        if not os.path.exists(FINAL_CSV) or os.path.getsize(FINAL_CSV) < 10:
            logger.error(f"{FINAL_CSV} æœªç”Ÿæˆæˆ–å†…å®¹æ— æ•ˆ")
            return None
        with open(FINAL_CSV, "r", encoding="utf-8") as f:
            lines = [line.strip() for line in f if line.strip()]
            node_count = len(lines) - 1 if lines else 0
            logger.info(f"{FINAL_CSV} åŒ…å« {node_count} ä¸ªèŠ‚ç‚¹")
        return FINAL_CSV
    except Exception as e:
        logger.error(f"æµ‹é€Ÿå¼‚å¸¸: {e}")
        return None

def filter_speed_and_deduplicate(csv_file: str):
    start_time = time.time()
    if not os.path.exists(csv_file):
        logger.info(f"{csv_file} ä¸å­˜åœ¨")
        return
    seen = set()
    final_rows = []
    try:
        with open(csv_file, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            header = next(reader, None)
            if not header:
                logger.error(f"{csv_file} æ²¡æœ‰æœ‰æ•ˆçš„è¡¨å¤´")
                return
            for row in reader:
                if len(row) < 2 or not row[0].strip():
                    continue
                key = (row[0], row[1])
                if key not in seen:
                    seen.add(key)
                    final_rows.append(row)
    except Exception as e:
        logger.error(f"æ— æ³•å¤„ç† {csv_file}: {e}")
        return
    if not final_rows:
        logger.info(f"æ²¡æœ‰æœ‰æ•ˆçš„èŠ‚ç‚¹")
        os.remove(csv_file)
        return
    try:
        final_rows.sort(key=lambda x: float(x[9]) if len(x) > 9 and x[9] and x[9].replace('.', '', 1).isdigit() else 0.0, reverse=True)
    except Exception as e:
        logger.warning(f"æ’åºå¤±è´¥: {e}")
    with open(csv_file, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        writer.writerows(final_rows)
    logger.info(f"{csv_file} å¤„ç†å®Œæˆï¼Œ{len(final_rows)} ä¸ªæ•°æ®èŠ‚ç‚¹ (è€—æ—¶: {time.time() - start_time:.2f} ç§’)")
    return len(final_rows)

def generate_ips_file(csv_file: str):
    start_time = time.time()
    if not os.path.exists(csv_file):
        logger.info(f"{csv_file} ä¸å­˜åœ¨")
        return
    country_cache = load_country_cache()
    final_nodes = []
    try:
        with open(csv_file, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            next(reader)
            for row in reader:
                if len(row) < 2:
                    continue
                ip, port = row[0], row[1]
                if not is_valid_ip(ip) or not is_valid_port(port):
                    continue
                country = country_cache.get(ip, '')
                if not country:
                    country = get_country_from_ip(ip, country_cache)
                final_nodes.append((ip, int(port), country))
    except Exception as e:
        logger.error(f"æ— æ³•è¯»å– {csv_file}: {e}")
        return
    if not final_nodes:
        logger.info(f"æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹")
        return
    country_count = defaultdict(int)
    labeled_nodes = []
    for ip, port, country in sorted(final_nodes, key=lambda x: x[2] or 'ZZ'):
        if country:
            country_count[country] += 1
            emoji, name = COUNTRY_LABELS.get(country, ('ğŸŒ', 'æœªçŸ¥'))
            label = f"{emoji}{name}-{country_count[country]}"
            labeled_nodes.append((ip, port, label))
    with open(IPS_FILE, "w", encoding="utf-8-sig") as f:
        for ip, port, label in labeled_nodes:
            f.write(f"{ip}:{port}#{label}\n")
    logger.info(f"ç”Ÿæˆ {IPS_FILE}ï¼Œ{len(labeled_nodes)} ä¸ªæ•°æ®èŠ‚ç‚¹ (è€—æ—¶: {time.time() - start_time:.2f} ç§’)")
    logger.info(f"å›½å®¶åˆ†å¸ƒ: {dict(country_count)}")
    save_country_cache(country_cache)
    return len(labeled_nodes)

def load_config() -> Dict[str, str]:
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                config = json.load(f)
                required_fields = ['user_name', 'user_email', 'repo_name', 'ssh_key_path', 'git_user_name']
                if all(field in config for field in required_fields):
                    logger.info("å·²ä»ç¼“å­˜åŠ è½½ Git é…ç½®")
                    return config
                else:
                    logger.warning("ç¼“å­˜æ–‡ä»¶ç¼ºå°‘å¿…è¦å­—æ®µï¼Œå°†é‡æ–°æç¤ºè¾“å…¥")
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½ç¼“å­˜æ–‡ä»¶ {CONFIG_FILE}: {e}")
    return {}

def save_config(config: Dict[str, str]):
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        os.chmod(CONFIG_FILE, stat.S_IRUSR | stat.S_IWUSR)
        logger.info(f"Git é…ç½®å·²ä¿å­˜åˆ° {CONFIG_FILE}")
    except Exception as e:
        logger.error(f"æ— æ³•ä¿å­˜ç¼“å­˜æ–‡ä»¶ {CONFIG_FILE}: {e}")
        sys.exit(1)

def generate_ssh_key() -> str:
    ssh_dir = os.path.expanduser("~/.ssh")
    private_key_path = SSH_KEY_PATH
    public_key_path = f"{private_key_path}.pub"

    if os.path.exists(private_key_path) and os.path.exists(public_key_path):
        logger.info(f"SSH å¯†é’¥å·²å­˜åœ¨: {private_key_path}")
        try:
            result = subprocess.run(
                ["ssh", "-T", "git@github.com"],
                capture_output=True,
                text=True,
                check=False
            )
            if "successfully authenticated" in result.stdout:
                logger.info("SSH å¯†é’¥éªŒè¯æˆåŠŸï¼Œå¯è¿æ¥åˆ° GitHub")
            else:
                logger.warning(f"SSH å¯†é’¥éªŒè¯å¤±è´¥: {result.stdout or result.stderr}")
                logger.info("è¯·ç¡®ä¿å…¬é’¥å·²æ·»åŠ åˆ° GitHub: https://github.com/settings/keys")
        except subprocess.CalledProcessError as e:
            logger.warning(f"æ— æ³•éªŒè¯ SSH è¿æ¥: {e.stderr}")
        return private_key_path  # ä¿®æ”¹ï¼šè¿”å›ç§é’¥è·¯å¾„

    try:
        os.makedirs(ssh_dir, mode=0o700, exist_ok=True)
        logger.info("æ­£åœ¨ç”Ÿæˆ SSH å¯†é’¥...")
        subprocess.run(
            ["ssh-keygen", "-t", "ed25519", "-f", private_key_path, "-N", ""],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        os.chmod(private_key_path, stat.S_IRUSR | stat.S_IWUSR)
        os.chmod(public_key_path, stat.S_IRUSR | stat.S_IWUSR)
        logger.info(f"SSH å¯†é’¥ç”ŸæˆæˆåŠŸ: {private_key_path}")

        with open(public_key_path, 'r', encoding='utf-8') as f:
            public_key = f.read().strip()
        logger.info("è¯·å°†ä»¥ä¸‹å…¬é’¥æ·»åŠ åˆ° GitHub SSH å¯†é’¥è®¾ç½® (https://github.com/settings/keys):")
        logger.info(public_key)
        logger.info("æ·»åŠ å®Œæˆåï¼ŒæŒ‰å›è½¦ç»§ç»­...")
        input()

        return private_key_path  # ä¿®æ”¹ï¼šè¿”å›ç§é’¥è·¯å¾„
    except subprocess.CalledProcessError as e:
        logger.error(f"ç”Ÿæˆ SSH å¯†é’¥å¤±è´¥: {e.stderr}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ç”Ÿæˆ SSH å¯†é’¥æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        sys.exit(1)

def setup_git_config() -> Dict[str, str]:
    config = load_config()
    if config:
        return config

    logger.info("æ£€æµ‹åˆ°æœ¬åœ°è¿è¡Œï¼Œéœ€è¦é…ç½® Git ä¿¡æ¯")
    user_name = input("è¯·è¾“å…¥ Git ç”¨æˆ·å: ").strip()
    while not user_name:
        logger.warning("ç”¨æˆ·åä¸èƒ½ä¸ºç©º")
        user_name = input("è¯·è¾“å…¥ Git ç”¨æˆ·å: ").strip()

    user_email = input("è¯·è¾“å…¥ Git é‚®ç®±: ").strip()
    while not user_email or '@' not in user_email:
        logger.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„é‚®ç®±åœ°å€")
        user_email = input("è¯·è¾“å…¥ Git é‚®ç®±: ").strip()

    repo_name = input("è¯·è¾“å…¥ GitHub ä»“åº“åç§°: ").strip()
    while not repo_name or '/' in repo_name:
        logger.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„ä»“åº“åç§°ï¼ˆä»…è¾“å…¥ä»“åº“åç§°ï¼Œä¾‹å¦‚ my-repoï¼‰")
        repo_name = input("è¯·è¾“å…¥ GitHub ä»“åº“åç§°: ").strip()

    try:
        result = subprocess.run(
            ["git", "config", "user.name"],
            capture_output=True,
            text=True,
            check=True
        )
        git_user_name = result.stdout.strip()
        if git_user_name:
            logger.info(f"æ£€æµ‹åˆ° Git é…ç½®çš„ç”¨æˆ·å: {git_user_name}")
        else:
            git_user_name = user_name
    except subprocess.CalledProcessError:
        logger.warning("æ— æ³•è·å– Git é…ç½®çš„ç”¨æˆ·åï¼Œå°†ä½¿ç”¨è¾“å…¥çš„ç”¨æˆ·å")
        git_user_name = user_name

    ssh_key_path = generate_ssh_key()

    config = {
        "user_name": user_name,
        "user_email": user_email,
        "repo_name": repo_name,
        "ssh_key_path": ssh_key_path,
        "git_user_name": git_user_name
    }
    save_config(config)
    return config

def initialize_git_repo():
    git_dir = os.path.join(os.getcwd(), ".git")
    if not os.path.exists(git_dir):
        logger.info("å½“å‰ç›®å½•ä¸æ˜¯ Git ä»“åº“ï¼Œæ‰§è¡Œ git init")
        try:
            subprocess.run(
                ["git", "init"],
                check=True,
                capture_output=True,
                text=True
            )
            logger.info("Git ä»“åº“åˆå§‹åŒ–æˆåŠŸ")
        except subprocess.CalledProcessError as e:
            logger.error(f"æ— æ³•åˆå§‹åŒ– Git ä»“åº“: {e.stderr}")
            return False
    return True

def detect_environment() -> tuple[str, bool, Dict[str, str]]:
    is_github_actions = os.getenv("GITHUB_ACTIONS") == "true"
    
    try:
        git_version = subprocess.run(
            ["git", "--version"],
            capture_output=True,
            text=True,
            check=True
        ).stdout.strip()
        logger.info(f"Git ç‰ˆæœ¬: {git_version}")
    except FileNotFoundError:
        logger.error("Git æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… Git (https://git-scm.com/downloads)")
        sys.exit(1)
    except subprocess.CalledProcessError as e:
        logger.error(f"æ— æ³•æ£€æµ‹ Git ç‰ˆæœ¬: {e.stderr}")
        sys.exit(1)

    if not is_github_actions:
        initialize_git_repo()

    try:
        branch = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True,
            text=True,
            check=True
        ).stdout.strip()
        if branch == "HEAD":
            logger.warning("å½“å‰å¤„äºåˆ†ç¦»å¤´çŠ¶æ€ï¼Œå°†å°è¯•åˆ‡æ¢åˆ°é»˜è®¤åˆ†æ”¯")
            try:
                default_branch = subprocess.run(
                    ["git", "remote", "show", "origin"],
                    capture_output=True,
                    text=True,
                    check=True
                ).stdout
                for line in default_branch.splitlines():
                    if "HEAD branch" in line:
                        branch = line.split(":")[-1].strip()
                        subprocess.run(
                            ["git", "checkout", branch],
                            check=True,
                            capture_output=True,
                            text=True
                        )
                        logger.info(f"å·²åˆ‡æ¢åˆ°é»˜è®¤åˆ†æ”¯: {branch}")
                        break
                else:
                    branch = "main"
                    logger.warning(f"æ— æ³•æ£€æµ‹è¿œç¨‹é»˜è®¤åˆ†æ”¯ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ”¯: {branch}")
                    subprocess.run(
                        ["git", "checkout", "-b", branch],
                        check=True,
                        capture_output=True,
                        text=True
                    )
                    logger.info(f"åˆ›å»ºå¹¶åˆ‡æ¢åˆ°æ–°åˆ†æ”¯: {branch}")
            except subprocess.CalledProcessError as e:
                branch = "main"
                logger.warning(f"æ— æ³•å¤„ç†åˆ†æ”¯åˆ‡æ¢: {e.stderr}ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ”¯: {branch}")
                subprocess.run(
                    ["git", "checkout", "-b", branch],
                    check=True,
                    capture_output=True,
                    text=True
                )
                logger.info(f"åˆ›å»ºå¹¶åˆ‡æ¢åˆ°æ–°åˆ†æ”¯: {branch}")
    except subprocess.CalledProcessError as e:
        branch = "main"
        logger.warning(f"æ— æ³•æ£€æµ‹å½“å‰åˆ†æ”¯: {e.stderr}ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ”¯: {branch}")
        try:
            subprocess.run(
                ["git", "checkout", "-b", branch],
                check=True,
                capture_output=True,
                text=True
            )
            logger.info(f"åˆ›å»ºå¹¶åˆ‡æ¢åˆ°æ–°åˆ†æ”¯: {branch}")
        except subprocess.CalledProcessError as e:
            logger.error(f"æ— æ³•åˆ›å»ºåˆ†æ”¯ {branch}: {e.stderr}")
            branch = "main"

    git_config = {}
    if not is_github_actions:
        git_config = setup_git_config()
        try:
            subprocess.run(
                ["git", "config", "--local", "user.name", git_config["user_name"]],
                check=True,
                capture_output=True,
                text=True
            )
            subprocess.run(
                ["git", "config", "--local", "user.email", git_config["user_email"]],
                check=True,
                capture_output=True,
                text=True
            )
            logger.info(f"å·²è®¾ç½® Git ç”¨æˆ·: {git_config['user_name']} <{git_config['user_email']}>")
        except subprocess.CalledProcessError as e:
            logger.warning(f"æ— æ³•è®¾ç½® Git ç”¨æˆ·é…ç½®: {e.stderr}. ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤")

        try:
            remote_url = subprocess.run(
                ["git", "remote", "get-url", "origin"],
                capture_output=True,
                text=True,
                check=True
            ).stdout.strip()
            if not remote_url:
                raise subprocess.CalledProcessError(1, "git remote get-url")
            if "github.com" not in remote_url.lower():
                logger.warning(f"è¿œç¨‹ä»“åº“åœ°å€ {remote_url} ä¸åƒæ˜¯ GitHub ä»“åº“")
        except subprocess.CalledProcessError:
            repo_name = git_config["repo_name"]
            git_user_name = git_config["git_user_name"]
            remote_url = f"git@github.com:{git_user_name}/{repo_name}.git"
            logger.info(f"è®¾ç½®è¿œç¨‹ä»“åº“åœ°å€ä¸º: {remote_url}")
            try:
                subprocess.run(
                    ["git", "remote", "add", "origin", remote_url],
                    check=True,
                    capture_output=True,
                    text=True
                )
            except subprocess.CalledProcessError as e:
                logger.warning(f"æ— æ³•è®¾ç½®è¿œç¨‹ä»“åº“åœ°å€: {e.stderr}. è·³è¿‡è¿œç¨‹æ“ä½œ")

        try:
            subprocess.run(
                ["git", "fetch", "origin"],
                check=True,
                capture_output=True,
                text=True
            )
            logger.info("æˆåŠŸæ‹‰å–è¿œç¨‹ä»“åº“")
        except subprocess.CalledProcessError as e:
            logger.warning(f"æ— æ³•æ‹‰å–è¿œç¨‹ä»“åº“: {e.stderr}. ç»§ç»­æœ¬åœ°æ“ä½œ")

    return branch, is_github_actions, git_config

def commit_and_push(branch: str, is_github_actions: bool):
    try:
        # æäº¤æ›´æ”¹
        subprocess.run(
            ["git", "add", IP_LIST_FILE, FINAL_CSV, IPS_FILE],
            check=True,
            capture_output=True,
            text=True
        )
        logger.info("å·²æ·»åŠ æ–‡ä»¶åˆ° Git æš‚å­˜åŒº")

        commit_message = f"Update IP data - {time.strftime('%Y-%m-%d %H:%M:%S')}"
        result = subprocess.run(
            ["git", "commit", "-m", commit_message],
            check=True,
            capture_output=True,
            text=True
        )
        logger.info(f"å·²æäº¤æ›´æ”¹: {commit_message}, è¾“å‡º: {result.stdout}")
    except subprocess.CalledProcessError as e:
        if "nothing to commit" in e.stderr:
            logger.info("æ²¡æœ‰éœ€è¦æäº¤çš„æ›´æ”¹ï¼Œæ— éœ€æ¨é€")
            return True  # ç›´æ¥è¿”å›ï¼Œé¿å…ä¸å¿…è¦çš„æ¨é€
        else:
            logger.warning(f"æ— æ³•æäº¤æ›´æ”¹: {e.stderr}")
            return False

    if not is_github_actions:
        try:
            # å…ˆæ‹‰å–è¿œç¨‹æ›´æ”¹å¹¶å°è¯•å˜åŸº
            logger.info(f"å°è¯•æ‹‰å–è¿œç¨‹åˆ†æ”¯å¹¶å˜åŸº: {branch}")
            result = subprocess.run(
                ["git", "pull", "--rebase", "origin", branch],
                check=True,
                capture_output=True,
                text=True
            )
            logger.info(f"æ‹‰å–å¹¶å˜åŸºæˆåŠŸ: {result.stdout}")
        except subprocess.CalledProcessError as e:
            logger.error(f"æ‹‰å–å¹¶å˜åŸºå¤±è´¥: {e.stderr}")
            logger.error("å¯èƒ½å­˜åœ¨å†²çªï¼Œè¯·æ‰‹åŠ¨è§£å†³åå†æ¨é€")
            return False

        try:
            # æ¨é€
            result = subprocess.run(
                ["git", "push", "origin", branch],
                check=True,
                capture_output=True,
                text=True
            )
            logger.info(f"å·²æ¨é€åˆ°è¿œç¨‹åˆ†æ”¯: {branch}, è¾“å‡º: {result.stdout}")
        except subprocess.CalledProcessError as e:
            logger.error(f"æ¨é€å¤±è´¥: {e.stderr}")
            logger.error("æ¨é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–åˆ†æ”¯çŠ¶æ€")
            return False

    return True

def main():
    parser = argparse.ArgumentParser(description="IP ç­›é€‰å’Œæµ‹é€Ÿå·¥å…·")
    parser.add_argument('--input', type=str, default=INPUT_FILE, help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--url', type=str, default=INPUT_URL, help="è¾“å…¥ URL")
    parser.add_argument('--offline', action='store_true', help="ç¦»çº¿æ¨¡å¼ï¼Œä½¿ç”¨æœ¬åœ° GeoIP æ•°æ®åº“ï¼Œä¸å°è¯•ä¸‹è½½")
    parser.add_argument('--update-geoip', action='store_true', help="å¼ºåˆ¶æ›´æ–° GeoIP æ•°æ®åº“")
    args = parser.parse_args()

    setup_and_activate_venv()
    # ä¼ é€’ update_geoip å‚æ•°ç»™ init_geoip_reader
    check_dependencies(offline=args.offline, update_geoip=args.update_geoip)

    branch, is_github_actions, git_config = detect_environment()

    input_file = args.input
    if args.url and not os.path.exists(input_file):
        if is_temp_file_valid(TEMP_FILE):
            input_file = TEMP_FILE
        else:
            input_file = fetch_and_save_to_temp_file(args.url)
            if not input_file:
                logger.error("æ— æ³•ä¸‹è½½è¾“å…¥æ–‡ä»¶ï¼Œé€€å‡º")
                sys.exit(1)

    ip_ports = extract_ip_ports_from_file(input_file)
    if not ip_ports:
        logger.error("æ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„ IP å’Œç«¯å£ï¼Œé€€å‡º")
        sys.exit(1)

    ip_list_file = write_ip_list(ip_ports)
    if not ip_list_file:
        logger.error("æ— æ³•ç”Ÿæˆ IP åˆ—è¡¨ï¼Œé€€å‡º")
        sys.exit(1)

    csv_file = run_speed_test()
    if not csv_file:
        logger.error("æµ‹é€Ÿå¤±è´¥ï¼Œé€€å‡º")
        sys.exit(1)

    node_count = filter_speed_and_deduplicate(csv_file)
    if not node_count:
        logger.error("è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆçš„èŠ‚ç‚¹ï¼Œé€€å‡º")
        sys.exit(1)

    final_node_count = generate_ips_file(csv_file)
    if not final_node_count:
        logger.error("æ— æ³•ç”Ÿæˆæœ€ç»ˆçš„ ips.txt æ–‡ä»¶ï¼Œé€€å‡º")
        sys.exit(1)

    if not commit_and_push(branch, is_github_actions):
        logger.warning("æäº¤æˆ–æ¨é€å¤±è´¥ï¼Œä½†æœ¬åœ°æ–‡ä»¶å·²ç”Ÿæˆ")

    logger.info("æµç¨‹å®Œæˆï¼")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œï¼Œé€€å‡º")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1)